# -*- coding: utf-8 -*-
"""(to use) FYP II.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bSNsX3Iun7aycjlyfmk6pKI3rdY_cS3J

# Load Data
"""

# Import libraries
import pandas as pd
import numpy as np

# Load dataset from local files

from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent
DATA_PATH = BASE_DIR / "data" / "conventional ventilation patient data v4.xlsx"

SHEET_NAME = "Sheet1"

df = pd.read_excel(DATA_PATH, sheet_name=SHEET_NAME)

"""# Preprocessing Data"""

# Create binary columns for current target status (in-range / out-of-range)
df["TV_in_range"] = ((df["TV"] >= df["Min_TV"]) & (df["TV"] <= df["Max_TV"])).astype(int)
df["ETCO2_in_range"] = ((df["ETCO2"] >= df["Min_ETCO2"]) & (df["ETCO2"] <= df["Max_ETCO2"])).astype(int)
df["SPO2_in_range"] = ((df["SPO2"] >= df["Min_SPO2"]) & (df["SPO2"] <= df["Max_SPO2"])).astype(int)
df["Pplat_in_range"] = (df["Pplat"] <= df["Max_Pplat"]).astype(int)

# Rename TIME (MINS) column
df.rename(columns={'TIME (MINS)': 'Time', 'NEW_ID': 'ID'}, inplace=True)
time_col = "Time"
groups = df["ID"]

# Create future target columns (target status at time t=t+one step)
# this means that _in_range_next is taken from .._in_range_current at time t + 15
in_range_cols = ["TV_in_range","ETCO2_in_range","SPO2_in_range","Pplat_in_range"]

for col in in_range_cols:
    df[col + "_next"] = df.groupby(groups)[col].shift(-1)

# drop last row per patient (because shifted target becomes NaN)
df = df.dropna(subset=[col + "_next" for col in in_range_cols])
target_cols = ["TV_in_range_next","ETCO2_in_range_next","SPO2_in_range_next","Pplat_in_range_next"]
# df[target_cols] = df[target_cols].astype(int)

groups = df["ID"]

# Drop unnecessary columns
drop_cols = ["OLD_ID", "TEMP", "Mean_MAP", "Mean_HR", "Mean_RR"]
df = df.drop(columns=drop_cols)

# Identify columns that have missing values, excluding the '15min_Success' column
contains_missing = df.columns[df.isnull().any()].tolist()
exclude_col = "15min_Success"
contains_missing = [col for col in contains_missing if col != exclude_col]

# Forward and backward fill missing values
df[contains_missing] = df.groupby(groups)[contains_missing].ffill()
df[contains_missing] = df.groupby(groups)[contains_missing].bfill()
df[contains_missing]

groups = df["ID"]

# Drop the '15min_Success' column
df = df.drop(columns='15min_Success')

# Drop other columns that have constant value throughout
min_max = ["Min_TV", "Max_TV", "Min_ETCO2", "Max_ETCO2", "Min_SPO2", "Max_SPO2", "Max_Pplat"]
drop_cols = [col for col in df.columns if df[col].nunique() == 1 and col not in min_max]
print(drop_cols)
df = df.drop(columns=drop_cols, errors="ignore")

# Reassign the ID column to groups, for dataset splitting
# groups = df["NEW_ID"]
df = df.drop(columns='ID')

# Checks
# Ensure groups still exist and align with df rows
try:
    len(groups)
except NameError:
    raise RuntimeError("`groups` variable not found.")

# Ensure lengths match:
if len(groups) != len(df):
    raise ValueError(f"Length mismatch: groups ({len(groups)}) vs df ({len(df)}).")

# Drop Diagnosis and Oxygen_Requirement_Prior_Intubation
df = df.drop(columns=["Diagnosis", "Oxygen_Requirement_Prior_Intubation"])

# Define target columns
target_cols = ["TV_in_range_next","ETCO2_in_range_next","SPO2_in_range_next","Pplat_in_range_next"]

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix
from sklearn.metrics import classification_report, multilabel_confusion_matrix
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer

# Convert IE_Ratio to numeric
def parse_ie_ratio(ie_ratio_str):
    try:
        a, b = ie_ratio_str.split(":")
        return float(b)/float(a)  # expiration / inspiration
    except:
        return np.nan

df["IE_Ratio_numeric"] = df["IE_Ratio"].apply(parse_ie_ratio)
df = df.drop(columns=["IE_Ratio"])

# df.columns[df.isnull().any()]

# Columns are categorized as categorical, binary, or numeric.
# time_col = "TIME (MINS)"
# min_max = ["Min_TV", "Max_TV", "Min_ETCO2", "Max_ETCO2", "Min_SPO2", "Max_SPO2", "Max_Pplat"]
binary_prefix = "Comorbid"  # all columns starting with this are assumed binary (0/1)
# Append binary column to 'binary_cols'.
binary_cols = [c for c in df.columns if c.startswith(binary_prefix)]
binary_cols.extend(in_range_cols)

# Specify categorical columns explicitly
# Triage_Initial, Premedication
categorical_cols = [c for c in ["GENDER","Diagnosis","Indication_Intubation", "Oxygen_Requirement_Prior_Intubation", "Induction_Agent",
                                "Paralytic_Agent","Stratified_Lung_Pathology","Sedation",
                                "Condition", "IE_Ratio"] if c in df.columns]


# Numeric features: everything numeric except target columns and binary_cols
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [c for c in numeric_cols if c not in target_cols + binary_cols + categorical_cols + min_max]

all_features = [c for c in df.columns if c not in target_cols + min_max]

# Variables for derived features
vars_main = ["TV", "ETCO2", "SPO2", "Pplat"]

# Matching min/max columns in the dataset
range_cols = {
    "TV": ("Min_TV", "Max_TV"),
    "ETCO2": ("Min_ETCO2", "Max_ETCO2"),
    "SPO2": ("Min_SPO2", "Max_SPO2"),
    "Pplat": (None, "Max_Pplat"),
}

# change in features
for v in vars_main:
    # absolute difference
    df[f"{v}_diff"] = df.groupby(groups)[v].diff().fillna(0)
    # relative change (%)
    df[f"{v}_pct_change"] = df.groupby(groups)[v].pct_change().fillna(0)

# distance-to-range features
# for each variable v:
# distance from lower bound,
# distance from upper bound,
# minimum distance (closest boundary)

for v in vars_main:
    low_col, high_col = range_cols[v]

    if low_col:  # only compute if low_col exists
        df[f"{v}_dist_low"] = df[v] - df[low_col]
    else:
        df[f"{v}_dist_low"] = np.nan  # or 0 if you prefer

    if high_col:
        df[f"{v}_dist_high"] = df[high_col] - df[v]
    else:
        df[f"{v}_dist_high"] = np.nan  # or 0

    df[f"{v}_dist_closest"] = df[
        [f"{v}_dist_low", f"{v}_dist_high"]
    ].min(axis=1)

df = df.drop(columns=['Pplat_dist_low'])

# lag features
# For the first time point (Time=0), lag is NaN. Can impute later if needed.
lag_vars = ["TV", "ETCO2", "SPO2", "Pplat", "HR", "RR"]
lag_cols = [f"{var}_lag1" for var in lag_vars]

for col in lag_cols:
    # Create lag
    df[col] = df.groupby(groups)[col.replace("_lag1", "")].shift(1)
    # Fill NaNs: first row per patient
    df[col] = df.groupby(groups)[col].bfill().ffill()

df.columns[df.isnull().any()]

# Drop min max range for TV, ETCO2, SPO2, Pplat
df = df.drop(columns=['Min_TV', 'Max_TV', 'Min_ETCO2', 'Max_ETCO2', 'Min_SPO2', 'Max_SPO2', 'Max_Pplat'])

# add new numeric features to numeric_cols
new_features = [c for c in df.columns if c not in all_features and c not in target_cols]
numeric_cols.extend(new_features)

# recompute all_features to include everything except targets
all_features = [c for c in df.columns if c not in target_cols]

# train/test/split

# Separate X (features) and y (target)
X = df.drop(columns=target_cols)
y = df[target_cols]

# Grouped train/test split using groups (split by patient)
from sklearn.model_selection import GroupShuffleSplit
splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
train_idx, test_idx = next(splitter.split(X, y, groups=groups))

X_train, X_test = X.iloc[train_idx].reset_index(drop=True), X.iloc[test_idx].reset_index(drop=True)
y_train, y_test = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)
groups_train, groups_test = groups.iloc[train_idx].reset_index(drop=True), groups.iloc[test_idx].reset_index(drop=True)

assert len(set(groups_train) & set(groups_test)) == 0, "Train and test sets have overlapping patients!"

print("Train patients:", groups_train.nunique(), "Test patients:", groups_test.nunique())
print("X_train shape:", X_train.shape, "X_test shape:", X_test.shape)

# preprocessing

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_cols),
        ("bin", "passthrough", binary_cols),
        ("num", MinMaxScaler(), numeric_cols),
    ],
    remainder="drop"
)

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed  = preprocessor.transform(X_test)

import pandas as pd

# Get feature names after OneHotEncoding
ohe_cols = preprocessor.named_transformers_["cat"].get_feature_names_out(categorical_cols)
all_cols = list(ohe_cols) + binary_cols + numeric_cols

X_train_processed = pd.DataFrame(X_train_processed, columns=all_cols)
X_test_processed  = pd.DataFrame(X_test_processed, columns=all_cols)

"""# Training and evaluating model"""

# Random Forest
from sklearn.multioutput import MultiOutputClassifier
rf = MultiOutputClassifier(RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced"))
rf.fit(X_train_processed, y_train)
y_pred = pd.DataFrame(rf.predict(X_test_processed), columns=y_test.columns)

# Calculate accuracy of models
from sklearn.metrics import accuracy_score

rf_accuracy = accuracy_score(y_test, y_pred)

print(f"RF Model Accuracy: {rf_accuracy}")

# Evaluation: classification report
print("=== Random Forest ===")
print(classification_report(y_test, y_pred, target_names=y_test.columns))

from sklearn.metrics import multilabel_confusion_matrix

# Function to print raw confusion matrices
def print_multilabel_confusion(y_true, y_pred, title_prefix=""):
    cms = multilabel_confusion_matrix(y_true, y_pred)
    labels = y_true.columns.tolist()
    for i, lab in enumerate(labels):
        print(f"{title_prefix}{lab} Confusion Matrix:")
        print(cms[i])  # raw 2x2 matrix
        print("TN, FP, FN, TP = ", cms[i].ravel())  # unpacked values
        print("-"*40)

# Example usage
print_multilabel_confusion(y_test, y_pred, title_prefix="RF - ")

# ROC AUC per label
# roc_auc_score supports multi-label with average=None to get per-label scores

try:
    if isinstance(rf.predict_proba(X_test_processed), list):
        y_score_rf = pd.DataFrame({col: prob[:,1] for col, prob in zip(y_test.columns, rf.predict_proba(X_test_processed))})
    else:
        y_score_rf = None
except Exception:
    y_score_rf = None

def per_label_auc(y_true, y_scores):
    aucs = {}
    for col in y_true.columns:
        try:
            aucs[col] = roc_auc_score(y_true[col], y_scores[col])
        except Exception as e:
            aucs[col] = np.nan
    return aucs

if y_score_rf is not None:
    print("RF AUCs:", per_label_auc(y_test, y_score_rf))

"""# Saving and exporting model"""

# define pipeline
from sklearn.pipeline import Pipeline
model_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', rf)
])

# save rf model
import joblib

joblib.dump(model_pipeline, ".venv\model/ventilation_model.pkl")

import json

with open(".venv\model/feature_names.json", "w") as f:
    json.dump(model_pipeline.feature_names_in_.tolist(), f)