# -*- coding: utf-8 -*-
"""model comparison (RF/LR/Tuned RF).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FUJMYgGoxi0-WxxCtDCunKS9vgtEPFEI
"""

# Import libraries
import pandas as pd
import numpy as np

from google.colab import files

uploaded = files.upload()  # This will prompt you to upload a file
file_name = list(uploaded.keys())[0]

df = pd.read_excel(file_name, sheet_name="Sheet1")
df.head()

# Create binary columns for current target status (in-range / out-of-range)
df["tv_in_range_current"] = (
    (df["tv"] >= df["min_tv"]) &
    (df["tv"] <= df["max_tv"])
).astype(int)

df["etco2_in_range_current"] = (
    (df["etco2"] >= df["min_etco2"]) &
    (df["etco2"] <= df["max_etco2"])
).astype(int)

df["spo2_in_range_current"] = (
    (df["spo2"] >= df["min_spo2"]) &
    (df["spo2"] <= df["max_spo2"])
).astype(int)

df["pplat_in_range_current"] = (
    df["pplat"] <= df["max_pplat"]
).astype(int)

df

# Keep patient ID separately so that filling missing values does not overstep other patient's data
groups = df["new_id"]
print("IDs: ", groups.nunique())

# Create target columns (target status at time t=t+one step)
# this means that _in_range_next is taken from .._in_range_current at time t + 15
df["tv_in_range_next"] = df.groupby(groups)["tv_in_range_current"].shift(-1)
df["etco2_in_range_next"] = df.groupby(groups)["etco2_in_range_current"].shift(-1)
df["spo2_in_range_next"] = df.groupby(groups)["spo2_in_range_current"].shift(-1)
df["pplat_in_range_next"] = df.groupby(groups)["pplat_in_range_current"].shift(-1)

"""# Cleaning dataset (drop and fill)"""

# Import libraries
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Drop unnecessary columns
# drop_cols = ["old_id", "temp", "mean_map", "mean_hr", "mean_rr"]
drop_cols = ["old_id", "temp"]
df = df.drop(columns=drop_cols)

# Identify columns that have missing values, excluding the '15min_Success' column
contains_missing = df.columns[df.isnull().any()].tolist()
exclude_col = "15min_success"
contains_missing = [col for col in contains_missing if col != exclude_col]
print(contains_missing)
df[contains_missing]

# Forward and backward fill missing values
df[contains_missing] = df.groupby(groups)[contains_missing].ffill()
df[contains_missing] = df.groupby(groups)[contains_missing].bfill()
df[contains_missing]
print("Remaining NaNs after cleaning:", df.isna().sum().sum())

df

# Drop rows that have missing values in "15min_success" column
# This is to make sure data only includes rows where patients are actually ventilated because some patients are extubated early.
df = df.dropna(subset=[exclude_col])
df

print("Remaining NaNs after cleaning:", df.isna().sum().sum())

# Drop the '15min_success' column
df = df.drop(columns=exclude_col)
df

# Drop other columns that have constant value throughout
min_max = ["min_tv", "max_tv", "min_etco2", "max_etco2", "min_spo2", "max_spo2", "max_pplat"]
drop_cols = [col for col in df.columns if df[col].nunique() == 1 and col not in min_max]
print(drop_cols)
df = df.drop(columns=drop_cols, errors="ignore")

# Reassign the ID column to groups before dropping the column, for dataset splitting
groups = df["new_id"]
print("IDs: ", len(groups))
df = df.drop(columns=["new_id"])

# Checks
# Ensure groups still exist and align with df rows
try:
    len(groups)
except NameError:
    raise RuntimeError("`groups` variable not found.")

# Ensure lengths match:
if len(groups) != len(df):
    raise ValueError(f"Length mismatch: groups ({len(groups)}) vs df ({len(df)}).")

# Drop diagnosis and oxygen_requirement_prior_intubation
# reason: dont know logic behind the numbers(classes) in the dataset
df = df.drop(columns=["diagnosis", "oxygen_requirement_prior_intubation"])

"""# Feature engineering"""

target_cols = ["tv_in_range_next", "etco2_in_range_next", "spo2_in_range_next", "pplat_in_range_next"]

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report, multilabel_confusion_matrix
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer

# Convert ie_ratio to numeric
def parse_ie_ratio(ie_ratio_str):
    try:
        a, b = ie_ratio_str.split(":")
        return float(b)/float(a)  # expiration / inspiration
    except:
        return np.nan

df["ie_ratio_numeric"] = df["ie_ratio"].apply(parse_ie_ratio)
df = df.drop(columns=["ie_ratio"])

df.columns[df.isnull().any()]

# Columns are categorized as categorical, binary, or numeric.
ir_suffix = "_in_range_current"
ir_cols = [c for c in df.columns if c.endswith(ir_suffix)]

binary_prefix = "comorbid"  # all columns starting with this are assumed binary (0/1)
# Append binary column to 'binary_cols'.
binary_cols = [c for c in df.columns if c.startswith(binary_prefix)]
binary_cols.extend(ir_cols)

# Specify categorical columns explicitly
# Triage_Initial, Premedication
categorical_cols = [c for c in ["gender", "indication_intubation", "induction_agent",
                                "paralytic_agent", "stratified_lung_pathology", "sedation",
                                "condition"] if c in df.columns]


# Numeric features: everything numeric except target columns and binary_cols
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [c for c in numeric_cols if c not in target_cols + binary_cols + categorical_cols + min_max]

all_features = [c for c in df.columns if c not in target_cols + min_max]

print("Binary columns:", binary_cols)
print("Categorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)
print("All features:", all_features)

# Variables for derived features
vars_main = ["tv", "etco2", "spo2", "pplat"]

# Matching min/max columns in the dataset
range_cols = {
    "tv": ("min_tv", "max_tv"),
    "etco2": ("min_etco2", "max_etco2"),
    "spo2": ("min_spo2", "max_spo2"),
    "pplat": (None, "max_pplat")
}

# change in features
for v in vars_main:
    # absolute difference
    df[f"{v}_diff"] = df.groupby(groups)[v].diff().fillna(0)
    # relative change (%)
    df[f"{v}_pct_change"] = df.groupby(groups)[v].pct_change().fillna(0)

# distance-to-range features
# for each variable v:
# distance from lower bound,
# distance from upper bound,
# minimum distance (closest boundary)

for v in vars_main:
    low_col, high_col = range_cols[v]

    if low_col:  # only compute if low_col exists
        df[f"{v}_dist_low"] = df[v] - df[low_col]
    else:
        df[f"{v}_dist_low"] = np.nan  # or 0 if you prefer

    if high_col:
        df[f"{v}_dist_high"] = df[high_col] - df[v]
    else:
        df[f"{v}_dist_high"] = np.nan  # or 0

    df[f"{v}_dist_closest"] = df[
        [f"{v}_dist_low", f"{v}_dist_high"]
    ].min(axis=1)

df = df.drop(columns=['pplat_dist_low'])

# lag features
# For the first time point (Time=0), lag is NaN.
lag_vars = ["tv", "etco2", "spo2", "pplat", "hr", "rr"]
lag_cols = [f"{var}_lag1" for var in lag_vars]

for col in lag_cols:
    var = col.replace("_lag1", "")
    df[col] = df.groupby(groups)[var].shift(1)
    # Fill first row lag (which is NaN) with current value
    df[col] = df[col].fillna(df[var])

df.columns[df.isnull().any()]

# Drop min max range for tv, etco2, spo2, pplat
df = df.drop(columns=["min_tv", "max_tv", "min_etco2", "max_etco2", "min_spo2", "max_spo2", "max_pplat"])

# add new numeric features to numeric_cols
new_features = [c for c in df.columns if c not in all_features and c not in target_cols]
numeric_cols.extend(new_features)

# recompute all_features to include everything except targets
all_features = [c for c in df.columns if c not in target_cols]

print("Binary columns:", binary_cols)
print("Categorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)
print("All features:", all_features)

"""# Preprocessing data and training model

Preprocessing data
"""

# Separate X (features) and y (target)
X = df.drop(columns=target_cols)
y = df[target_cols]

# Grouped train/test split using groups (split by patient)
from sklearn.model_selection import GroupShuffleSplit
splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
train_idx, test_idx = next(splitter.split(X, y, groups=groups))

X_train, X_test = X.iloc[train_idx].reset_index(drop=True), X.iloc[test_idx].reset_index(drop=True)
y_train, y_test = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)
groups_train, groups_test = groups.iloc[train_idx].reset_index(drop=True), groups.iloc[test_idx].reset_index(drop=True)

assert len(set(groups_train) & set(groups_test)) == 0, "Train and test sets have overlapping patients!"

print("Train patients:", groups_train.nunique(), "Test patients:", groups_test.nunique())
print("X_train shape:", X_train.shape, "X_test shape:", X_test.shape)

# preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_cols),
        ("bin", "passthrough", binary_cols),
        ("num", MinMaxScaler(), numeric_cols),
    ],
    remainder="drop"
)

"""Training model"""

# Random Forest
from sklearn.multioutput import MultiOutputClassifier
rf = MultiOutputClassifier(RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced"))

# correction: 15.1.26
# define pipeline
from sklearn.pipeline import Pipeline
model_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', rf)
])

# fit pipeline on RAW data
model_pipeline.fit(X_train, y_train)

# Test
y_pred = pd.DataFrame(
    model_pipeline.predict(X_test),
    columns=y_test.columns
)

"""# Evaluating model"""

# Evaluation: classification report
print("=== Random Forest ===")
print(classification_report(y_test, y_pred, target_names=y_test.columns))

from sklearn.metrics import multilabel_confusion_matrix

# Function to print raw confusion matrices
def print_multilabel_confusion(y_true, y_pred, title_prefix=""):
    cms = multilabel_confusion_matrix(y_true, y_pred)
    labels = y_true.columns.tolist()
    for i, lab in enumerate(labels):
        print(f"{title_prefix}{lab} Confusion Matrix:")
        print(cms[i])  # raw 2x2 matrix
        print("TN, FP, FN, TP = ", cms[i].ravel())  # unpacked values
        print("-"*40)

print_multilabel_confusion(y_test, y_pred, title_prefix="RF - ")

# ROC AUC score per label
from sklearn.metrics import roc_auc_score

def get_multilabel_auc(pipeline, X_test, y_test):
    y_proba_list = pipeline.named_steps["classifier"].predict_proba(
        pipeline.named_steps["preprocessor"].transform(X_test)
    )

    y_scores = pd.DataFrame(index=y_test.index)
    for col, prob in zip(y_test.columns, y_proba_list):
        if prob.shape[1] == 2:
            y_scores[col] = prob[:, 1]
        else:
            y_scores[col] = np.nan

    return per_label_auc(y_test, y_scores), y_scores

# Function to compute per-label AUC safely
def per_label_auc(y_true, y_scores):
    aucs = {}
    for col in y_true.columns:
        if y_true[col].nunique() < 2:
            aucs[col] = np.nan
        else:
            aucs[col] = roc_auc_score(y_true[col], y_scores[col])
    return aucs

# Compute
aucs_rf, y_score_rf = get_multilabel_auc(model_pipeline, X_test, y_test)
# Re-run RF AUC
print("\n=== Random Forest Per-Label ROC AUC ===")
for k, v in aucs_rf.items():
    print(f"{k}: {v:.3f}")

"""# Logistic regression"""

from sklearn.linear_model import LogisticRegression

lr = MultiOutputClassifier(
    LogisticRegression(
        max_iter=1000,
        class_weight="balanced",
        solver="liblinear"
    )
)

lr_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', lr)
])

lr_pipeline.fit(X_train, y_train)

y_pred_lr = pd.DataFrame(
    lr_pipeline.predict(X_test),
    columns=y_test.columns
)

print("=== Logistic Regression ===")
print(classification_report(y_test, y_pred_lr, target_names=y_test.columns))
print_multilabel_confusion(y_test, y_pred_lr, title_prefix="LR - ")

# Compute
aucs_lr, y_score_lr = get_multilabel_auc(lr_pipeline, X_test, y_test)
print("\n=== LogReg Per-Label ROC AUC ===")
for k, v in aucs_lr.items():
    print(f"{k}: {v:.3f}")

auc_comparison = pd.DataFrame({
    "RandomForest": aucs_rf,
    "LogisticRegression": aucs_lr
})

print("\n=== ROC AUC Comparison ===")
print(auc_comparison)

"""# Tuned RF"""

# class_weight = {0: 3, 1: 1}
rf_tuned = MultiOutputClassifier(
    RandomForestClassifier(
        n_estimators=250,
        max_depth=8,              # LIMIT tree depth
        min_samples_leaf=10,       # force generalization
        min_samples_split=10,
        max_features="sqrt",       # decorrelate trees
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    )
)

# n_estimators=300,
# max_depth=12,              # LIMIT tree depth
# min_samples_leaf=10,       # force generalization
# min_samples_split=20,
# max_features="sqrt",       # decorrelate trees
# class_weight="balanced",
# random_state=42,
# n_jobs=-1

rf_pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", rf_tuned)
])

rf_pipeline.fit(X_train, y_train)

# Test
y_pred_tuned = pd.DataFrame(
    rf_pipeline.predict(X_test),
    columns=y_test.columns
)

print("=== Tuned RF ===")
print(classification_report(y_test, y_pred_tuned, target_names=y_test.columns))
print_multilabel_confusion(y_test, y_pred_tuned, title_prefix="Tuned RF - ")

# Compute
aucs_tuned, y_score_tuned = get_multilabel_auc(rf_pipeline, X_test, y_test)

print("\n=== Tuned RF Per-Label ROC AUC ===")
for k, v in aucs_tuned.items():
    print(f"{k}: {v:.3f}")

auc_comparison = pd.DataFrame({
    "LogisticRegression": aucs_lr,
    "TunedRF": aucs_tuned
})

print("\n=== ROC AUC Comparison ===")
print(auc_comparison)

threshold = 0.8  # stricter

spo2_prob = y_score_rf["spo2_in_range_next"]
spo2_pred_thresh = (spo2_prob >= threshold).astype(int)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test["spo2_in_range_next"], spo2_pred_thresh)
print(cm)