# -*- coding: utf-8 -*-
"""FYP 2 (IR).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o-9EpIlT-FN7lrXfzeMNSSM1IMusYOqS

Model: Random Forest

Column Name Convention: snake_case

0: out of range (oor)

1: in range

# Loading dataset
"""

# Import libraries
import pandas as pd
import numpy as np

# Load dataset from local files

from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent
DATA_PATH = BASE_DIR / "data" / "conventional ventilation patient data v5.xlsx"
SHEET_NAME = "Sheet1"

df = pd.read_excel(DATA_PATH, sheet_name=SHEET_NAME)

"""# Creating new columns (in range status and target columns)"""

df

# Create binary columns for current target status (in-range / out-of-range)
df["tv_in_range_current"] = (
    (df["tv"] >= df["min_tv"]) &
    (df["tv"] <= df["max_tv"])
).astype(int)

df["etco2_in_range_current"] = (
    (df["etco2"] >= df["min_etco2"]) &
    (df["etco2"] <= df["max_etco2"])
).astype(int)

df["spo2_in_range_current"] = (
    (df["spo2"] >= df["min_spo2"]) &
    (df["spo2"] <= df["max_spo2"])
).astype(int)

df["pplat_in_range_current"] = (
    df["pplat"] <= df["max_pplat"]
).astype(int)

df

# Keep patient ID separately so that filling missing values does not overstep other patient's data
groups = df["new_id"]
print("IDs: ", groups.nunique())

# Create target columns (target status at time t=t+one step)
# this means that _in_range_next is taken from .._in_range_current at time t + 15
df["tv_in_range_next"] = df.groupby(groups)["tv_in_range_current"].shift(-1)
df["etco2_in_range_next"] = df.groupby(groups)["etco2_in_range_current"].shift(-1)
df["spo2_in_range_next"] = df.groupby(groups)["spo2_in_range_current"].shift(-1)
df["pplat_in_range_next"] = df.groupby(groups)["pplat_in_range_current"].shift(-1)

"""# Cleaning dataset (drop and fill)"""

# Import libraries
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Drop unnecessary columns
# drop_cols = ["old_id", "temp", "mean_map", "mean_hr", "mean_rr"]
drop_cols = ["old_id", "temp"]
df = df.drop(columns=drop_cols)

# Identify columns that have missing values, excluding the '15min_Success' column
contains_missing = df.columns[df.isnull().any()].tolist()
exclude_col = "15min_success"
contains_missing = [col for col in contains_missing if col != exclude_col]
print(contains_missing)
df[contains_missing]

# Forward and backward fill missing values
df[contains_missing] = df.groupby(groups)[contains_missing].ffill()
df[contains_missing] = df.groupby(groups)[contains_missing].bfill()
df[contains_missing]
print("Remaining NaNs after cleaning:", df.isna().sum().sum())

df

# Drop rows that have missing values in "15min_success" column
# This is to make sure data only includes rows where patients are actually ventilated because some patients are extubated early.
df = df.dropna(subset=[exclude_col])
df

print("Remaining NaNs after cleaning:", df.isna().sum().sum())

# Drop the '15min_success' column
df = df.drop(columns=exclude_col)
df

# Drop other columns that have constant value throughout
min_max = ["min_tv", "max_tv", "min_etco2", "max_etco2", "min_spo2", "max_spo2", "max_pplat"]
drop_cols = [col for col in df.columns if df[col].nunique() == 1 and col not in min_max]
print(drop_cols)
df = df.drop(columns=drop_cols, errors="ignore")

# Reassign the ID column to groups before dropping the column, for dataset splitting
groups = df["new_id"]
print("IDs: ", len(groups))
df = df.drop(columns=["new_id"])

# Checks
# Ensure groups still exist and align with df rows
try:
    len(groups)
except NameError:
    raise RuntimeError("`groups` variable not found.")

# Ensure lengths match:
if len(groups) != len(df):
    raise ValueError(f"Length mismatch: groups ({len(groups)}) vs df ({len(df)}).")

# Drop diagnosis and oxygen_requirement_prior_intubation
# reason: dont know logic behind the numbers(classes) in the dataset
df = df.drop(columns=["diagnosis", "oxygen_requirement_prior_intubation"])

"""# Feature engineering"""

target_cols = ["tv_in_range_next", "etco2_in_range_next", "spo2_in_range_next", "pplat_in_range_next"]

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report, multilabel_confusion_matrix
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer

# Convert ie_ratio to numeric
def parse_ie_ratio(ie_ratio_str):
    try:
        a, b = ie_ratio_str.split(":")
        return float(b)/float(a)  # expiration / inspiration
    except:
        return np.nan

df["ie_ratio_numeric"] = df["ie_ratio"].apply(parse_ie_ratio)
df = df.drop(columns=["ie_ratio"])

df.columns[df.isnull().any()]

# Columns are categorized as categorical, binary, or numeric.
ir_suffix = "_in_range_current"
ir_cols = [c for c in df.columns if c.endswith(ir_suffix)]

binary_prefix = "comorbid"  # all columns starting with this are assumed binary (0/1)
# Append binary column to 'binary_cols'.
binary_cols = [c for c in df.columns if c.startswith(binary_prefix)]
binary_cols.extend(ir_cols)

# Specify categorical columns explicitly
# Triage_Initial, Premedication
categorical_cols = [c for c in ["gender", "indication_intubation", "induction_agent",
                                "paralytic_agent", "stratified_lung_pathology", "sedation",
                                "condition"] if c in df.columns]


# Numeric features: everything numeric except target columns and binary_cols
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
numeric_cols = [c for c in numeric_cols if c not in target_cols + binary_cols + categorical_cols + min_max]

all_features = [c for c in df.columns if c not in target_cols + min_max]

print("Binary columns:", binary_cols)
print("Categorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)
print("All features:", all_features)

# Variables for derived features
vars_main = ["tv", "etco2", "spo2", "pplat"]

# Matching min/max columns in the dataset
range_cols = {
    "tv": ("min_tv", "max_tv"),
    "etco2": ("min_etco2", "max_etco2"),
    "spo2": ("min_spo2", "max_spo2"),
    "pplat": (None, "max_pplat")
}

# change in features
for v in vars_main:
    # absolute difference
    df[f"{v}_diff"] = df.groupby(groups)[v].diff().fillna(0)
    # relative change (%)
    df[f"{v}_pct_change"] = df.groupby(groups)[v].pct_change().fillna(0)

# distance-to-range features
# for each variable v:
# distance from lower bound,
# distance from upper bound,
# minimum distance (closest boundary)

for v in vars_main:
    low_col, high_col = range_cols[v]

    if low_col:  # only compute if low_col exists
        df[f"{v}_dist_low"] = df[v] - df[low_col]
    else:
        df[f"{v}_dist_low"] = np.nan  # or 0 if you prefer

    if high_col:
        df[f"{v}_dist_high"] = df[high_col] - df[v]
    else:
        df[f"{v}_dist_high"] = np.nan  # or 0

    df[f"{v}_dist_closest"] = df[
        [f"{v}_dist_low", f"{v}_dist_high"]
    ].min(axis=1)

df = df.drop(columns=['pplat_dist_low'])

# lag features
# For the first time point (Time=0), lag is NaN.
lag_vars = ["tv", "etco2", "spo2", "pplat", "hr", "rr"]
lag_cols = [f"{var}_lag1" for var in lag_vars]

for col in lag_cols:
    var = col.replace("_lag1", "")
    df[col] = df.groupby(groups)[var].shift(1)
    # Fill first row lag (which is NaN) with current value
    df[col] = df[col].fillna(df[var])

df.columns[df.isnull().any()]

# Drop min max range for tv, etco2, spo2, pplat
df = df.drop(columns=["min_tv", "max_tv", "min_etco2", "max_etco2", "min_spo2", "max_spo2", "max_pplat"])

# add new numeric features to numeric_cols
new_features = [c for c in df.columns if c not in all_features and c not in target_cols]
numeric_cols.extend(new_features)

# recompute all_features to include everything except targets
all_features = [c for c in df.columns if c not in target_cols]

print("Binary columns:", binary_cols)
print("Categorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)
print("All features:", all_features)

"""# Preprocessing data and training model

Preprocessing data
"""

# Separate X (features) and y (target)
X = df.drop(columns=target_cols)
y = df[target_cols]

# Grouped train/test split using groups (split by patient)
from sklearn.model_selection import GroupShuffleSplit
splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)
train_idx, test_idx = next(splitter.split(X, y, groups=groups))

X_train, X_test = X.iloc[train_idx].reset_index(drop=True), X.iloc[test_idx].reset_index(drop=True)
y_train, y_test = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)
groups_train, groups_test = groups.iloc[train_idx].reset_index(drop=True), groups.iloc[test_idx].reset_index(drop=True)

assert len(set(groups_train) & set(groups_test)) == 0, "Train and test sets have overlapping patients!"

print("Train patients:", groups_train.nunique(), "Test patients:", groups_test.nunique())
print("X_train shape:", X_train.shape, "X_test shape:", X_test.shape)

# preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_cols),
        ("bin", "passthrough", binary_cols),
        ("num", MinMaxScaler(), numeric_cols),
    ],
    remainder="drop"
)

"""Training model"""

# Random Forest
from sklearn.multioutput import MultiOutputClassifier
rf = MultiOutputClassifier(RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced"))

# correction: 15.1.26
# define pipeline
from sklearn.pipeline import Pipeline
model_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', rf)
])

# fit pipeline on RAW data
model_pipeline.fit(X_train, y_train)

# Test
y_pred = pd.DataFrame(
    model_pipeline.predict(X_test),
    columns=y_test.columns
)
# end of correction: 15.1.26

"""# Evaluating model"""

# Evaluation: classification report
print("=== Random Forest ===")
print(classification_report(y_test, y_pred, target_names=y_test.columns))

from sklearn.metrics import multilabel_confusion_matrix

# Function to print raw confusion matrices
def print_multilabel_confusion(y_true, y_pred, title_prefix=""):
    cms = multilabel_confusion_matrix(y_true, y_pred)
    labels = y_true.columns.tolist()
    for i, lab in enumerate(labels):
        print(f"{title_prefix}{lab} Confusion Matrix:")
        print(cms[i])  # raw 2x2 matrix
        print("TN, FP, FN, TP = ", cms[i].ravel())  # unpacked values
        print("-"*40)

print_multilabel_confusion(y_test, y_pred, title_prefix="RF - ")

# ROC AUC per label
# roc_auc_score supports multi-label with average=None to get per-label scores

try:
    if isinstance(rf.predict_proba(X_test_processed), list):
        y_score_rf = pd.DataFrame({col: prob[:,1] for col, prob in zip(y_test.columns, rf.predict_proba(X_test_processed))})
    else:
        y_score_rf = None
except Exception:
    y_score_rf = None

def per_label_auc(y_true, y_scores):
    aucs = {}
    for col in y_true.columns:
        try:
            aucs[col] = roc_auc_score(y_true[col], y_scores[col])
        except Exception as e:
            aucs[col] = np.nan
    return aucs

if y_score_rf is not None:
    print("RF AUCs:", per_label_auc(y_test, y_score_rf))

"""# Saving and exporting model"""

# save rf model
import joblib

joblib.dump(model_pipeline, r"model\ventilation_model_v2.pkl")

# save feature names
import json

with open(r"model\feature_names_v2.json", "w") as f:
    json.dump(model_pipeline.feature_names_in_.tolist(), f)